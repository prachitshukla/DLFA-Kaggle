{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "VyKd4-O27sAS",
        "53g0zVbjRV7K"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prachitshukla/DLFA-Kaggle/blob/development/M4_Mini_Hackathon_To_Perform_Classification_of_Coronavirus_Tweets_PS_GRU_0.77.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: To perform text classification of coronavirus tweets during the peak Covid - 19 period using LSTMs/RNNs/CNNs/BERT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU, BERT) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/db0ea322e4b14ad1b29d14fbe406d4e5) and open your user settings page. Click Account.\n",
        "\n",
        "* Click on your profile picture at the top-right corner of the page.\n",
        "\n",
        "![alt text](https://i.imgur.com/kSLmEj2.png)\n",
        "\n",
        "* In the popout menu, click the Settings option.\n",
        "\n",
        "![alt text](https://i.imgur.com/tNi6yun.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n",
        "![alt text](https://i.imgur.com/vRNBgrF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "#!pip uninstall urllib3\n",
        "#!pip install urllib3>=1.26.11\n",
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 List of installed pakage"
      ],
      "metadata": {
        "id": "VyKd4-O27sAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "OH4JiGGr74Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c perform-classification-of-coronavirus-tweets"
      ],
      "metadata": {
        "id": "BYJqsfpFk5n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/perform-classification-of-coronavirus-tweets.zip"
      ],
      "metadata": {
        "id": "jZRrRMaz90Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* install gensim"
      ],
      "metadata": {
        "id": "TxnpyI-4_Acp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the below upgraded installations and restart the session\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "QcS3uImAK8ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import tensorflow as tf  # use TensorFlow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import chardet\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the positive and negative files and split the sentences into a list\n",
        "with open('corona_nlp_test.csv/corona_nlp_test.csv',\"rb\") as data_test:\n",
        "  result = chardet.detect(data_test.read())\n",
        "  print(result)\n",
        "  data_test_set = pd.read_csv('corona_nlp_test.csv/corona_nlp_test.csv', encoding=result['encoding'])\n",
        "\n",
        "with open('corona_nlp_train.csv/corona_nlp_train.csv',\"rb\") as data_train:\n",
        "  result = chardet.detect(data_train.read())\n",
        "  print(result)\n",
        "  data_train_set = pd.read_csv('corona_nlp_train.csv/corona_nlp_train.csv', encoding=result['encoding'])"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* check first 5 records of train dataframe"
      ],
      "metadata": {
        "id": "rUI14yJ8NFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.head())"
      ],
      "metadata": {
        "id": "t7YiYySENJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.isnull().sum())"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set[\"Sentiment\"])"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution bar plot\n",
        "sentiment_count = data_train_set['Sentiment'].value_counts()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sentiment_count.index, sentiment_count.values)\n",
        "plt.title('Sentiments Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AA73RMIz-StY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "sns.countplot(data=data_train_set, x=data_train_set['Location'],  order= data_train_set['Location'].value_counts().iloc[:10].index)\n",
        "plt.title('Top 10 Countries with the Highest Tweet Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "sentiment_count={}\n",
        "for sentiment in data_train_set['Sentiment'].unique():\n",
        "  sentiment_count[sentiment]=data_train_set['Sentiment'].value_counts()[sentiment]\n",
        "  #print(sentiment,data_train_set['Sentiment'].value_counts()[sentiment])\n",
        "plt.pie(sentiment_count.values(), labels=sentiment_count.keys(), autopct='%1.1f%%')\n",
        "plt.title('Distribution of Sentiments')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentiment in data_train_set['Sentiment'].unique():\n",
        "  plt.figure(figsize=(20,5))\n",
        "  text=' '.join(data_train_set['OriginalTweet'].astype(str))\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.title(sentiment, fontweight='bold')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* convert categorical lables to numerical values"
      ],
      "metadata": {
        "id": "e4aQj93LLwAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "sentiments = [\"Extremely Positive\", \"Positive\",  \"Neutral\", \"Negative\", \"Extremely Negative\"]\n",
        "le.fit(sentiments)\n",
        "sentiment_to_num = {\"Extremely Positive\": 0,\"Positive\": 1,\"Neutral\": 2, \"Negative\": 3,\"Extremely Negative\": 4}\n",
        "data_train_set['Sentiment_num'] = data_train_set['Sentiment'].map(sentiment_to_num)\n",
        "data_train_set.head()"
      ],
      "metadata": {
        "id": "eXV8v859L3qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* function to preprocess the data"
      ],
      "metadata": {
        "id": "rIwuBr1nxJP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_set['OriginalTweet'] = data_train_set['OriginalTweet'].apply(lambda x:simple_preprocess(x, max_len=30))\n"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data_train_set['OriginalTweet'] = data_train_set['OriginalTweet'].apply(lambda x: [w for w in x if not w in stop_words])\n"
      ],
      "metadata": {
        "id": "vFEOiEVtPObo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_set.head()"
      ],
      "metadata": {
        "id": "QKNFnryyPcpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameters"
      ],
      "metadata": {
        "id": "cvLoWbuMPaHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "MAX_SENT_LEN = 30   # Number of words to consider from each review\n",
        "MAX_VOCAB_SIZE = 50000  # Max vocabulary size\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 30"
      ],
      "metadata": {
        "id": "Jzn6N17jPj9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize and Pad"
      ],
      "metadata": {
        "id": "cCVazLKVP2b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in data_train_set['OriginalTweet']])\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "_2d5Yv4DP5i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in data_train_set['OriginalTweet']])\n",
        "X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "y = data_train_set['Sentiment_num']\n",
        "#y_onehot = to_categorical(y, num_classes=5)\n",
        "#y_onehot\n"
      ],
      "metadata": {
        "id": "-fPKbRKfQoe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting data to train and test set"
      ],
      "metadata": {
        "id": "LVvovBjFQyTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, stratify=y_onehot, random_state=42, train_size=30000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, train_size=30000)"
      ],
      "metadata": {
        "id": "yxJRxlh1Q1kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "UZZ2YpuYRhvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import GloVe Embedding Files"
      ],
      "metadata": {
        "id": "MFay7W9y66jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "ipython.magic(\"sx unzip glove.6B.zip\")"
      ],
      "metadata": {
        "id": "EMATcPo26_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "f = open('/content/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "kF6Ed3I_SgYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)"
      ],
      "metadata": {
        "id": "c2jPtHvTR4mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "3RBJPpvPR9JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "vczKAk5eR9-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras (4 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sequential model by stacking neural net units\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim,\n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = MAX_SENT_LEN,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.5, name='first_gru_layer')))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True, dropout=0.5, name='second_gru_layer')))\n",
        "#model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.5, name='second_gru_layer')))\n",
        "#model.add(GRU(128, return_sequences=True, dropout=0.3, name='first_gru_layer'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(128, name='third_gru_layer', dropout=0.5)))\n",
        "#model.add(Bidirectional(GRU(64, name='second_gru_layer', dropout=0.5)))\n",
        "#model.add(GRU(64, name='second_gru_layer', dropout=0.3))\n",
        "#model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(5, activation='sigmoid', name='output_layer'))\n",
        "model.add(Dense(5, activation='softmax', name='output_layer'))"
      ],
      "metadata": {
        "id": "2Mgz2XbHS8s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train Model"
      ],
      "metadata": {
        "id": "EG6gAPUNUc6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model.compile(loss='categorical_crossentropy',\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FQJkKkwgUfw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',           # or 'val_accuracy'\n",
        "    patience=3,                   # stop after 3 epochs without improvement\n",
        "    restore_best_weights=True,   # roll back to best weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "H8RA51U326o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary of the built model...')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_bGLvmgoTAdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrain Model on Full dataset"
      ],
      "metadata": {
        "id": "hsM3ElA90qV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y)\n",
        "print('Summary of the built model...')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "dc6efJy10w-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n"
      ],
      "metadata": {
        "id": "tUPKgMwtU65s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "p5Ffu2uLyoZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepaire Testing Set\n",
        "data_test_set['OriginalTweet'] = data_test_set['OriginalTweet'].apply(lambda x:simple_preprocess(x, max_len=30))\n",
        "\n",
        "data_test_set['OriginalTweet'] = data_test_set['OriginalTweet'].apply(lambda x: [w for w in x if not w in stop_words])\n",
        "\n",
        "#Convert Test Set\n",
        "X_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in data_test_set['OriginalTweet']])\n",
        "X_test  = pad_sequences(X_test, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0U6UzAg8Q-uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the prediction on the test data after training\n",
        "test_predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "SrnEl2fNRz2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the max values from prediction\n",
        "predicted_test_classes = np.argmax(test_predictions, axis=1)\n",
        "print(predicted_test_classes)\n",
        "categorical_map={0:\"Extremely Positive\",1:\"Positive\",2:\"Neutral\",3:\"Negative\",4:\"Extremely Negative\"}\n",
        "predicted_test_sentiments = [categorical_map[i] for i in predicted_test_classes]\n",
        "\n",
        "# From the max values decode the sentiments\n",
        "#predicted_test_sentiments = le.inverse_transform(predicted_test_classes)\n",
        "#sentiment_to_num = {\"Extremely Positive\": 0,\"Positive\": 1,\"Neutral\": 2, \"Negative\": 3,\"Extremely Negative\": 4}\n",
        "\n",
        "#print(\"Predicted sentiments (first 10):\", predicted_test_sentiments[:10])\n",
        "\n",
        "print(predicted_test_sentiments)"
      ],
      "metadata": {
        "id": "d5K-O3YsTDk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the test IDs\n",
        "test_ids = data_test_set['UserName'].tolist()\n",
        "\n",
        "# Create a dataframe and add the Test_Id and predicted sentiment text to it\n",
        "output_df = pd.DataFrame({'Test_Id': test_ids, 'Sentiment': predicted_test_sentiments})\n",
        "\n",
        "# Write to CSV file\n",
        "output_df.to_csv('sentiment_predictions_AdditionalLayer_fulltrain_higher_dim.csv', index=False)"
      ],
      "metadata": {
        "id": "Fv3Y2l-4Ssit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the Test_Id which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."
      ]
    }
  ]
}