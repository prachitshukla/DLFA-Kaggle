{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "VyKd4-O27sAS",
        "53g0zVbjRV7K"
      ],
      "gpuType": "T4",
      "name": "M4_Mini_Hackathon_to_Perform_Classification_of_Coronavirus_Tweets_PS_BertTweet.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prachitshukla/DLFA-Kaggle/blob/development/M4_Mini_Hackathon_to_Perform_Classification_of_Coronavirus_Tweets_PS_BertTweet_5Epoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: To perform text classification of coronavirus tweets during the peak Covid - 19 period using LSTMs/RNNs/CNNs/BERT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU, BERT) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/db0ea322e4b14ad1b29d14fbe406d4e5) and open your user settings page. Click Account.\n",
        "\n",
        "* Click on your profile picture at the top-right corner of the page.\n",
        "\n",
        "![alt text](https://i.imgur.com/kSLmEj2.png)\n",
        "\n",
        "* In the popout menu, click the Settings option.\n",
        "\n",
        "![alt text](https://i.imgur.com/tNi6yun.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n",
        "![alt text](https://i.imgur.com/vRNBgrF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "#!pip uninstall urllib3\n",
        "#!pip install urllib3>=1.26.11\n",
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 List of installed pakage"
      ],
      "metadata": {
        "id": "VyKd4-O27sAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "OH4JiGGr74Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c perform-classification-of-coronavirus-tweets"
      ],
      "metadata": {
        "id": "BYJqsfpFk5n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/perform-classification-of-coronavirus-tweets.zip"
      ],
      "metadata": {
        "id": "jZRrRMaz90Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* install gensim"
      ],
      "metadata": {
        "id": "TxnpyI-4_Acp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the below upgraded installations and restart the session\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "QcS3uImAK8ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import tensorflow as tf  # use TensorFlow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import chardet\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the positive and negative files and split the sentences into a list\n",
        "with open('corona_nlp_test.csv/corona_nlp_test.csv',\"rb\") as data_test:\n",
        "  result = chardet.detect(data_test.read())\n",
        "  print(result)\n",
        "  data_test_set = pd.read_csv('corona_nlp_test.csv/corona_nlp_test.csv', encoding=result['encoding'])\n",
        "\n",
        "with open('corona_nlp_train.csv/corona_nlp_train.csv',\"rb\") as data_train:\n",
        "  result = chardet.detect(data_train.read())\n",
        "  print(result)\n",
        "  data_train_set = pd.read_csv('corona_nlp_train.csv/corona_nlp_train.csv', encoding=result['encoding'])"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* check first 5 records of train dataframe"
      ],
      "metadata": {
        "id": "rUI14yJ8NFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.head())"
      ],
      "metadata": {
        "id": "t7YiYySENJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.isnull().sum())"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set[\"Sentiment\"])"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution bar plot\n",
        "sentiment_count = data_train_set['Sentiment'].value_counts()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sentiment_count.index, sentiment_count.values)\n",
        "plt.title('Sentiments Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AA73RMIz-StY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "sns.countplot(data=data_train_set, x=data_train_set['Location'],  order= data_train_set['Location'].value_counts().iloc[:10].index)\n",
        "plt.title('Top 10 Countries with the Highest Tweet Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "sentiment_count={}\n",
        "for sentiment in data_train_set['Sentiment'].unique():\n",
        "  sentiment_count[sentiment]=data_train_set['Sentiment'].value_counts()[sentiment]\n",
        "  #print(sentiment,data_train_set['Sentiment'].value_counts()[sentiment])\n",
        "plt.pie(sentiment_count.values(), labels=sentiment_count.keys(), autopct='%1.1f%%')\n",
        "plt.title('Distribution of Sentiments')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentiment in data_train_set['Sentiment'].unique():\n",
        "  plt.figure(figsize=(20,5))\n",
        "  text=' '.join(data_train_set['OriginalTweet'].astype(str))\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.title(sentiment, fontweight='bold')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting data to train and test set"
      ],
      "metadata": {
        "id": "LVvovBjFQyTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Sentiments to respective Numbers"
      ],
      "metadata": {
        "id": "pxmerGJwzdPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Sentiment to Number\n",
        "sentiments = [\"Extremely Positive\", \"Positive\",  \"Neutral\", \"Negative\", \"Extremely Negative\"]\n",
        "sentiment_to_num = {\"Extremely Positive\": 0,\"Positive\": 1,\"Neutral\": 2, \"Negative\": 3,\"Extremely Negative\": 4}\n",
        "data_train_set['label'] = data_train_set['Sentiment'].map(sentiment_to_num)\n",
        "\n",
        "\n",
        "print(data_train_set.head())"
      ],
      "metadata": {
        "id": "ksmvwceC2HJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create sub dataset"
      ],
      "metadata": {
        "id": "J0DRII4RKtZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 'OriginalTweet' as input text\n",
        "train_dataset = Dataset.from_pandas(data_train_set[['OriginalTweet', 'label']])\n",
        "test_dataset = Dataset.from_pandas(data_test_set[['OriginalTweet']])"
      ],
      "metadata": {
        "id": "NClqhIN1-tlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create auto tokezizer"
      ],
      "metadata": {
        "id": "T6XrH42JKwt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(data_train_set['Sentiment'].unique()))\n"
      ],
      "metadata": {
        "id": "Dsh7apBr_AmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize the dataset"
      ],
      "metadata": {
        "id": "4EcWEkJhK1aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['OriginalTweet'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "pkckLrvMzgQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format the tokenize dataset"
      ],
      "metadata": {
        "id": "KwLrj-XV2fQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer needs specific format\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
      ],
      "metadata": {
        "id": "NyxBuXxMLXE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Evaluation Matrix"
      ],
      "metadata": {
        "id": "E_VRIC5XMB5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
      ],
      "metadata": {
        "id": "IPcZDuMS_0k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Hyperparameters/Training Strategy"
      ],
      "metadata": {
        "id": "6blyrL_EMvSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"no\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=5, # Tried with 3\n",
        "    save_strategy=\"no\",\n",
        "    #logging_dir=\"./logs\",\n",
        "    #logging_steps=10,\n",
        "    report_to=[]\n",
        ")\n"
      ],
      "metadata": {
        "id": "MUIgAr0R_2op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras (4 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "HHw3bbm0NBm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "qXE-nxEA_8fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "nnmRJLeY__Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n"
      ],
      "metadata": {
        "id": "tUPKgMwtU65s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inferences against trained model/Validate Model"
      ],
      "metadata": {
        "id": "nmRSr0pPNgN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = predictions.predictions.argmax(axis=1)\n",
        "\n",
        "sentiment_map={0:\"Extremely Positive\",1:\"Positive\",2:\"Neutral\",3:\"Negative\",4:\"Extremely Negative\"}\n",
        "\n",
        "\n",
        "#test_df['Sentiment'] = [id2label[label] for label in predicted_test_sentiments]"
      ],
      "metadata": {
        "id": "lpUe-bpaADJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Model"
      ],
      "metadata": {
        "id": "hMBwOiD2MsYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction to sentiment\n",
        "data_test_set['Sentiment']  = [sentiment_map[i] for i in predicted_labels]\n",
        "\n",
        "# Get the test IDs\n",
        "test_ids = data_test_set['UserName'].tolist()\n",
        "\n",
        "# Create a dataframe and add the Test_Id and predicted sentiment text to it\n",
        "output_df = pd.DataFrame({'Test_Id': test_ids, 'Sentiment': data_test_set['Sentiment']})\n",
        "\n",
        "# Write to CSV file\n",
        "output_df.to_csv('sentiment_predictions_BertT_5Epochs.csv', index=False)"
      ],
      "metadata": {
        "id": "yJNeMxd5Aiqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "p5Ffu2uLyoZj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the Test_Id which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."
      ]
    }
  ]
}